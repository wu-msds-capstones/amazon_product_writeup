# Results

To evaluate the predictive power of our engineered features in explaining `product_order`, we applied three distinct modeling approaches: ordinary least squares (OLS) linear regression, Lasso regression with cross-validation for regularization, and a Random Forest regressor to capture non-linear relationships and interactions. All models were trained using the finalized engineered feature set, which incorporates category-based rankings, interaction metrics, brand popularity, semantic similarity, and other derived indicators of product visibility and relevance. Model performance was assessed using two key metrics: the root mean squared error (RMSE), which measures the average magnitude of prediction errors, and the coefficient of determination (R²), which quantifies the proportion of variance in `product_order` explained by the model. This combination of models and metrics provides complementary perspectives that balance interpretability, feature influence analysis, and the ability to capture more complex patterns in the data.

### Linear Regression

The linear regression model produced a test RMSE of 25.27 and an R² of 0.18, indicating that the model explains only about 18% of the variance in `product_order`. While this level of explanatory power is modest, the model provides useful insights into the direction and relative magnitude of feature–target relationships. Because linear regression assumes additive, linear effects and no complex feature interactions, its primary value here lies in interpretability rather than predictive accuracy.

The coefficients reveal patterns consistent with expectations. Products marked as in_stock tend to appear higher in search rankings, as indicated by the negative coefficient (-8.46), meaning being in stock is associated with an average improvement of roughly eight positions in `product_order`. Similarly, higher stars ratings are linked to better rankings (-3.79). Features such as `percentage_saving` show a positive coefficient (2.42), suggesting that the larger the discount, the higher your product will be ranked (recall `percentage_saving` values are negative). Semantic alignment between title and category (`semantic_sim`) and stronger brand presence (`log_brand_popularity`) are also negatively associated with `product_order`, indicating higher placement when these scores are higher. These results are visualized in @fig-basic-feat.

![Coefficients from the linear regression model estimating product_order as a function of engineered features. Negative coefficients (orange) indicate features associated with higher search result placement (lower product_order values), while positive coefficients (blue) indicate association with lower placement. The strongest influence comes from in_stock, stars, and percentage_saving, highlighting the importance of availability, customer ratings, and discounting in determining product ranking.](images/regression_features.png){#fig-basic-feat}

While the model’s performance metrics highlight its limitations in capturing the full complexity of Amazon’s ranking algorithm, linear regression’s transparency makes it well-suited for exploratory analysis. It offers a straightforward benchmark for comparing more complex models, helps identify the most influential features, and provides interpretable coefficients that align with domain expectations.

### Lasso Regression

The Lasso regression model was tuned with five-fold cross-validation. It produced an R² of 0.177 and an RMSE of 25.39, essentially the same as the unregularized linear regression. This indicates that the modest explanatory power is not the result of overfitting, but rather the limited ability of linear relationships to explain `product_order`.

Regularization significantly altered the coefficient structure, as shown in @fig-lasso-feat. Features such as `percentage_saving`, `stars`, `high_quality`, and `semantic_sim` were reduced entirely to zero, meaning the model determined they did not contribute enough predictive value to justify retaining them under the penalty. Other coefficients, like `in_stock`, `log_brand_popularity`, and the category rank features, were retained but shrunk in magnitude compared to the standard linear regression. The largest surviving effect was from `in_stock` (-3.19), though this is substantially smaller than the -8.46 observed in the unregularized model.

![Feature coefficients from the Lasso regression model predicting product_order. Regularization has shrunk several coefficients entirely to zero, indicating their limited contribution under penalization. The remaining features, especially in_stock and log_brand_popularity, retain negative coefficients, suggesting their strong influence on higher product placement (lower product_order values). This sparsity highlights Lasso’s emphasis on simpler, more stable models by penalizing weaker predictors.](images/lasso_features.png){#fig-lasso-feat}

This shrinkage reflects Lasso’s conservative approach: by eliminating weaker predictors and dampening the influence of the rest, it produces a more stable model less sensitive to noise. However, in this case, regularization came at the cost of discarding features that might still contribute in a non-linear setting, and it did not improve predictive performance. This reinforces the idea that to improve fit, we will need models capable of capturing interactions and non-linear effects beyond what a sparse linear framework can represent.

### Random Forest Regression

The Random Forest model achieved an R² of 0.133 and an RMSE of 26.06, indicating slightly lower explanatory power and higher prediction error compared to the linear models. This suggests that while the algorithm can capture non-linear relationships and interactions among features, the current feature set still limits predictive accuracy. The drop in R² also highlights that ensemble tree methods may not outperform simpler models when the signal-to-noise ratio is low or when relevant features are missing.

Feature importance rankings provide insight into how the model made its predictions, displayed in @fig-rf-feat. `category_rank_positive_interaction` emerged as the most influential predictor (0.253), followed by `category_rank_orders` (0.187) and `semantic_sim` (0.146). `log_brand_popularity` (0.137) and `category_rank_price` (0.135) were also important, suggesting that brand strength, pricing position, and high interaction with the product are key drivers in predicting product placement. Variables such as `in_stock` (0.008) and `high_quality` (0.011) contributed little to the model, likely because their effects are either already captured by other correlated features or are not strongly tied to search ranking in this dataset.

![Feature importances from the Random Forest regression model predicting product_order. The most influential predictors were category_rank_positive_interaction and category_rank_orders, followed by semantic_sim and log_brand_popularity. These results suggest that consumer interaction signals, recent product demand, and semantic alignment with category labels are key drivers of search result ranking. In contrast, binary indicators such as in_stock and high_quality had minimal impact in this non-linear model.](images/rf_feat_importance.png){#fig-rf-feat}

One advantage of Random Forest is its ability to model complex, non-linear interactions without the assumptions of linear regression. However, in this case, the modest performance gain over simpler models was absent, reinforcing that feature limitations, rather than model choice, may be the primary bottleneck. Nonetheless, the importance scores provide a useful check, confirming that the most influential features align with domain expectations.
